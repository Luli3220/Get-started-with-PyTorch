{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 是一个开源的深度学习框架，以其灵活性和动态计算图而广受欢迎。\n",
    "\n",
    "#### PyTorch 主要有以下几个基础概念：张量（Tensor）、自动求导（Autograd）、神经网络模块（nn.Module）、优化器（optim）等。\n",
    "\n",
    "- 张量（Tensor）：PyTorch 的核心数据结构，支持多维数组，并可以在 CPU 或 GPU 上进行加速计算。\n",
    "- 自动求导（Autograd）：PyTorch 提供了自动求导功能，可以轻松计算模型的梯度，便于进行反向传播和优化。\n",
    "- 神经网络（nn.Module）：PyTorch 提供了简单且强大的 API 来构建神经网络模型，可以方便地进行前向传播和模型定义。\n",
    "- 优化器（Optimizers）：使用优化器（如 Adam、SGD 等）来更新模型的参数，使得损失最小化。\n",
    "- 设备（Device）：可以将模型和张量移动到 GPU 上以加速计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量（Tensor）\n",
    "张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。\n",
    "\n",
    "张量可以视为一个多维数组，支持加速计算的操作。\n",
    "\n",
    "在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。\n",
    "\n",
    "- 维度（Dimensionality）：张量的维度指的是数据的多维数组结构。\n",
    "\n",
    "- 形状（Shape）：张量的形状是指每个维度上的大小。\n",
    "\n",
    "- 数据类型（Dtype）：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如torch.int8、torch.int32）、浮点型（如torch.float32、torch.float64）和布尔型（torch.bool）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[-0.9742, -0.4071,  0.2809],\n",
      "        [-0.5351,  0.1305,  0.9044]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[-1.2567,  0.8586, -0.8735],\n",
      "        [-0.8498,  0.8757,  0.3063]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "#创建一个 2*3 全为0的张量\n",
    "a=torch.zeros(2,3)\n",
    "print(a)\n",
    "\n",
    "#创建一个2*3 全为1的张量\n",
    "b=torch.ones(2,3)\n",
    "print(b)\n",
    "\n",
    "#创建一个2*3的随机数张量\n",
    "c=torch.randn(2,3)\n",
    "print(c)\n",
    "\n",
    "#从numpy中创建张量\n",
    "numpy_array=np.array([[1,2],[3,4]])\n",
    "tensor_from_numpy=torch.from_numpy(numpy_array)\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "#在指定设备（CPU/GPU）上创建张量\n",
    "device=torch.device('cuda'if torch.cuda.is_available() else \"cpu\")\n",
    "d=torch.randn(2,3,device=device)\n",
    "print(d)\n",
    "\n",
    "#在指"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的张量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1819,  0.3639,  0.5964],\n",
      "        [-1.0485,  1.4671,  0.5222]])\n",
      "tensor([[ 0.3291, -1.0766, -1.7806],\n",
      "        [ 0.2579, -1.2362, -0.4447]])\n",
      "tensor([[-0.8528, -0.7127, -1.1842],\n",
      "        [-0.7906,  0.2308,  0.0774]])\n",
      "tensor([[-0.3890, -0.3917, -1.0619],\n",
      "        [-0.2704, -1.8137, -0.2322]])\n",
      "tensor([[-1.1819, -1.0485],\n",
      "        [ 0.3639,  1.4671],\n",
      "        [ 0.5964,  0.5222]])\n",
      "tensor([[-1.1819, -1.0485],\n",
      "        [ 0.3639,  1.4671],\n",
      "        [ 0.5964,  0.5222]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#张量相加\n",
    "e=torch.randn(2,3)\n",
    "f=torch.randn(2,3)\n",
    "print(e)\n",
    "print(f)\n",
    "print(e+f)\n",
    "\n",
    "#逐元素乘法\n",
    "print(e*f)\n",
    "\n",
    "#张量的转置\n",
    "print(e.t()) \n",
    "#或者\n",
    "print(e.transpose(0,1))\n",
    "\n",
    "#放回张量的形状  【注意】 这里不需要括号\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度和自动微分\n",
    "PyTorch的张量支持自动微分，这是深度学习中的关键特性。当你创建一个需要梯度的张量时，PyTorch可以自动计算其梯度：\n",
    "\n",
    "torch.Tensor 对象有一个 requires_grad 属性，用于指示是否需要计算该张量的梯度。\n",
    "\n",
    "当你创建一个 requires_grad=True 的张量时，PyTorch 会自动跟踪所有对它的操作，以便在之后计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求导（Autograd）\n",
    "自动求导（Automatic Differentiation，简称Autograd）是深度学习框架中的一个核心特性，它允许计算机自动计算数学函数的导数。\n",
    "\n",
    "在深度学习中，自动求导主要用于两个方面：一是在训练神经网络时计算梯度，二是进行反向传播算法的实现。\n",
    "\n",
    "自动求导基于链式法则（Chain Rule），这是一个用于计算复杂函数导数的数学法则。链式法则表明，复合函数的导数是其各个组成部分导数的乘积。在深度学习中，模型通常是由许多层组成的复杂函数，自动求导能够高效地计算这些层的梯度。\n",
    "\n",
    "PyTorch 提供了自动求导功能，通过 autograd 模块来自动计算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9052, -0.2560],\n",
      "        [-0.4051,  1.0275]], requires_grad=True)\n",
      "tensor([[1.0948, 1.7440],\n",
      "        [1.5949, 3.0275]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 3.5959,  9.1251],\n",
      "        [ 7.6315, 27.4965]], grad_fn=<MulBackward0>)\n",
      "tensor(11.9623, grad_fn=<MeanBackward0>)\n",
      "tensor([[1.6422, 2.6161],\n",
      "        [2.3924, 4.5412]])\n"
     ]
    }
   ],
   "source": [
    "#创建一个需要计算梯度的张量\n",
    "x=torch.randn(2,2,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y=x+2\n",
    "print(y)\n",
    "\n",
    "z=y*y*3\n",
    "print(z)\n",
    "\n",
    "out=z.mean()\n",
    "print(out)\n",
    "\n",
    "#反向传播，计算梯度  #自动计算out对于每一个x的梯度\n",
    "out.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停止梯度计算\n",
    "\n",
    "如果你不希望某些张量的梯度被计算（例如，当你不需要反向传播时），可以使用 torch.no_grad() 或设置 requires_grad=False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 torch.no_grad() 禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    y = x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络（nn.Module）\n",
    "神经网络是一种模仿人脑神经元连接的计算模型，由多层节点（神经元）组成，用于学习数据之间的复杂模式和关系。\n",
    "\n",
    "神经网络通过调整神经元之间的连接权重来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。\n",
    "\n",
    "神经网络的类型包括前馈神经网络、卷积神经网络（CNN）、循环神经网络（RNN）和长短期记忆网络（LSTM），它们在图像识别、语音处理、自然语言处理等多个领域都有广泛应用。\n",
    "\n",
    "PyTorch 提供了一个非常方便的接口来构建神经网络模型，即 torch.nn.Module。\n",
    "\n",
    "我们可以继承 nn.Module 类并定义自己的网络层。\n",
    "\n",
    "创建一个简单的神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#定义一个简单的全连接神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNN,self).__init__()  # 调用父类的构造函数\n",
    "    self.fc1=nn.Linear(2,2)  #输入层到隐藏层\n",
    "    self.fc2=nn.Linear(2,1)  #隐藏层到输出层\n",
    "  def forward(self,x):\n",
    "    x=torch.relu(self.fc1(x)) #ReLU激活函数\n",
    "    x=self.fc2(x)\n",
    "    return x\n",
    "#创建网络实例\n",
    "model=SimpleNN()\n",
    "\n",
    "#打印模型的结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程：\n",
    "\n",
    "- 前向传播（Forward Propagation）： 在前向传播阶段，输入数据通过网络层传递，每层应用权重和激活函数，直到产生输出。\n",
    "\n",
    "- 计算损失（Calculate Loss）： 根据网络的输出和真实标签，计算损失函数的值。\n",
    "\n",
    "- 反向传播（Backpropagation）： 反向传播利用自动求导技术计算损失函数关于每个参数的梯度。\n",
    "\n",
    "- 参数更新（Parameter Update）： 使用优化器根据梯度更新网络的权重和偏置。\n",
    "\n",
    "- 迭代（Iteration）： 重复上述过程，直到模型在训练数据上的性能达到满意的水平。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播与损失计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1284]], grad_fn=<AddmmBackward0>)\n",
      "tensor(2.0430, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1,2)\n",
    "\n",
    "#forward\n",
    "output=model(x)\n",
    "print(output)\n",
    "\n",
    "#损失函数(这里使用MSE)\n",
    "criterion=nn.MSELoss()\n",
    "\n",
    "#假设目标为 1\n",
    "target=torch.randn(1,1)\n",
    "\n",
    "loss=criterion(output,target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器（Optimizers）\n",
    "优化器在训练过程中更新神经网络的参数，以减少损失函数的值。\n",
    "\n",
    "PyTorch 提供了多种优化器，例如 SGD、Adam 等。\n",
    "\n",
    "使用优化器进行参数更新："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化器(Adam)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练步骤\n",
    "optimizer.zero_grad()  #清空梯度\n",
    "loss.backward() #反向传播\n",
    "optimizer()  #更新参数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看个案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0675\n",
      "Epoch [20/100], Loss: 1.0531\n",
      "Epoch [30/100], Loss: 1.0397\n",
      "Epoch [40/100], Loss: 1.0276\n",
      "Epoch [50/100], Loss: 1.0161\n",
      "Epoch [60/100], Loss: 1.0051\n",
      "Epoch [70/100], Loss: 0.9946\n",
      "Epoch [80/100], Loss: 0.9845\n",
      "Epoch [90/100], Loss: 0.9748\n",
      "Epoch [100/100], Loss: 0.9654\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器\n",
    "\n",
    "# 4. 假设我们有训练数据 X 和 Y\n",
    "X = torch.randn(10, 2)  # 10 个样本，2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标值\n",
    "\n",
    "# 5. 训练循环\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    \n",
    "    # 每 10 轮输出一次损失\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设备（Device）\n",
    "PyTorch 允许你将模型和数据移动到 GPU 上进行加速。\n",
    "\n",
    "使用 torch.device 来指定计算设备。\n",
    "\n",
    "将模型和数据移至 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 将模型移动到设备\n",
    "model.to(device)\n",
    "\n",
    "# 将数据移动到设备\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
